{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fada19e7507740f38c7a10d7341cf9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the main points covered in the video transcript, along with relevant code snippets where applicable:\n",
      "\n",
      "1. Introduction to Transformer-based language models (GPT, BERT, etc.):\n",
      "- Revolutionized natural language processing (NLP)\n",
      "- Captures long-range dependencies in input text using self-attention mechanisms\n",
      "- Advantages over traditional recurrent neural networks (RNNs):\n",
      "  - Better parallelization and scalability\n",
      "  - Code snippet: Demonstrates how to load and generate text using a pre-trained GPT-2 model with the Hugging Face Transformers library\n",
      "\n",
      "2. Recent advancements in generative AI models (GPT-3, LLaMA):\n",
      "- Capable of generating human-like text based on prompts or input data\n",
      "- Potential applications in creative writing, code generation, and scientific research\n",
      "- Code snippet: Demonstrates how to generate text using a pre-trained GPT-3 model with the Hugging Face Transformers library\n",
      "\n",
      "3. Ethical considerations surrounding generative AI models:\n",
      "- Potential for misuse, bias, and spread of misinformation\n",
      "- Importance of responsible development and deployment of these models\n",
      "- Need for transparency and accountability\n",
      "\n",
      "4. Future of NLP and generative AI:\n",
      "- Development of multimodal models that can process and generate different types of data (text, images, videos, etc.)\n",
      "- Code snippet: Demonstrates how to load and generate text using a pre-trained multimodal model with the Hugging Face Transformers library\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def summarize(path=r'F:\\works\\A-important\\A-neurals\\GenAi-Verse\\yt-video-summarization\\Data\\my_file.txt'):\n",
    "    model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=r'D:\\hugging-models')\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", cache_dir=r'D:\\hugging-models')\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Please summarize the following transcript with bullet points and provide any relevant code snippets if necessary, it should be so easy to understand:\"},\n",
    "        {\"role\": \"assistant\", \"content\": transcript}\n",
    "    ]\n",
    "\n",
    "    def get_formatted_input(messages, context=\"\"):\n",
    "        system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "        conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "        formatted_input = system + \"\\n\\n\" + conversation\n",
    "\n",
    "        return formatted_input\n",
    "\n",
    "    formatted_input = get_formatted_input(messages)\n",
    "    tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=tokenized_prompt.input_ids,\n",
    "        attention_mask=tokenized_prompt.attention_mask,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=eos_token_id\n",
    "    )\n",
    "\n",
    "    response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "    summary = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    print(summary)\n",
    "\n",
    "summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{1: '334'}\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1:\"334\"}\n",
    "\n",
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
